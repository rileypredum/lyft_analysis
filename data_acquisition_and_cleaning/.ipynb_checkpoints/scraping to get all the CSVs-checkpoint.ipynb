{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this notebook\n",
    "\n",
    "The goal of this notebook is to pull all the raw CSV files for this project from the directory on AWS. This is the original notebook that got me the datasets.\n",
    "\n",
    "This code is re-used in the master_df_cleaning notebook but this is the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Move to the data folder for this project\n",
    "os.chdir('C:/Users/riley/Documents/Coding/DSC/datas/baywheels-data/')\n",
    "\n",
    "# Get the main URL and turn it into XML soup\n",
    "r = requests.get(\"https://s3.amazonaws.com/baywheels-data/\")\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data, \"xml\")\n",
    "\n",
    "# The data file URLs are in the <Key> node so find that\n",
    "keys = soup.find_all('Key')\n",
    "\n",
    "key_list = []\n",
    "\n",
    "# Grab the name of each file by taking the text attribute of the key node\n",
    "for key in keys:\n",
    "    key_list.append(key.text)\n",
    "    \n",
    "# Slice off the trailing file, index.html\n",
    "key_list_final = key_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads all files in the list of filenames.\n",
    "# Deal with extracting them in the next cell.\n",
    "\n",
    "import zipfile, urllib.request, shutil\n",
    "\n",
    "for key in key_list_final:\n",
    "\n",
    "    url = 'https://s3.amazonaws.com/baywheels-data/' + key\n",
    "    file_name = key\n",
    "\n",
    "    with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all items ending in .zip in the dir_name, \n",
    "# extract their contents to dir_name and delete the original zip folder.\n",
    "\n",
    "extension = \".zip\"\n",
    "dir_name = 'C:/Users/riley/Documents/Coding/DSC/datas/baywheels-data'\n",
    "\n",
    "for item in os.listdir(dir_name): # loop through items in dir\n",
    "    if item.endswith(extension): # check for \".zip\" extension\n",
    "        file_name = os.path.abspath(item) # get full path of files\n",
    "        zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "        zip_ref.extractall(dir_name) # extract file to dir\n",
    "        zip_ref.close() # close file\n",
    "        os.remove(file_name) # delete zipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all CSVs in the directory to iterate over and make\n",
    "# dataframes from.\n",
    "\n",
    "dir_name = 'C:/Users/riley/Documents/Coding/DSC/datas/baywheels-data/'\n",
    "\n",
    "# List of all .csv filenames to be read into dataframes\n",
    "csv_list = os.listdir(dir_name)\n",
    "\n",
    "# Build a list of dataframe names by removing .csv\n",
    "df_names = []\n",
    "\n",
    "for csv in csv_list:\n",
    "    df_name = csv.replace('.csv', '')\n",
    "    df_names.append(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of DFs and the filenames\n",
    "df_dict_names = dict(zip(df_names, csv_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary of dataframes to then concatenate into one dataframe\n",
    "\n",
    "dict_of_dataframes = {}\n",
    "for i in range(len(df_names)):\n",
    "    dict_of_dataframes[df_names[i]] = pd.read_csv(csv_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\riley\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "master_df = pd.concat(dict_of_dataframes.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
